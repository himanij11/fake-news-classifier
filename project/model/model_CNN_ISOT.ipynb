{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12785146248356746092\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_VNUM = 20000\n",
    "SEQUENCE_LENGTH = 300\n",
    "EMBED_DIMENSION = 100\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"lemmatized_clean_text\", \"labels\"]\n",
    "data = pd.read_csv(\"../Data_Preprocessing/Preprocessed_Dataset/ISOT_Preprocessed_Data.csv\", encoding='utf-8', encoding_errors='ignore')[column_names]\n",
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = keras.preprocessing.text.Tokenizer()\n",
    "token.fit_on_texts(data[column_names[0]])\n",
    "vocab_size = len(token.word_index) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:08, 44728.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 131638/131638 [00:00<00:00, 2231206.06it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "# for line in tqdm(f):\n",
    "#     value = line.split(' ')\n",
    "#     word = value[0]\n",
    "#     coef = np.array(value[1:],dtype = 'float32')\n",
    "#     embedding_vector[word] = coef\n",
    "for line in tqdm(f):\n",
    "    word, coefs = line.split(maxsplit=1)\n",
    "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "    embedding_vector[word] = coefs\n",
    "embedding_matrix = np.zeros((vocab_size, EMBED_DIMENSION))\n",
    "for word,i in tqdm(token.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 100)          13164000  \n",
      "                                                                 \n",
      " permute (Permute)           (None, 100, 300)          0         \n",
      "                                                                 \n",
      " Conv1D (Conv1D)             (None, 96, 128)           192128    \n",
      "                                                                 \n",
      " MaxPooling1D (MaxPooling1D)  (None, 48, 128)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6144)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6145      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,362,273\n",
      "Trainable params: 198,273\n",
      "Non-trainable params: 13,164,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inputs = keras.Input(shape=(None,), name=\"input\")\n",
    "# x = layers.Embedding(input_dim=vocab_size, output_dim=EMBED_DIMENSION, \n",
    "#                      embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
    "#                      input_length=SEQUENCE_LENGTH, trainable = False)(inputs)\n",
    "# x = layers.Conv1D(128, 5, activation='relu', use_bias=False, name=\"Conv1D\")(x)\n",
    "# x = layers.MaxPooling1D(pool_size=2, name=\"MaxPooling1D\")(x)\n",
    "# x = layers.LSTM(32, activation=None, use_bias=False, name=\"LSTM\")(x)\n",
    "# outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "# model = keras.Model(inputs, outputs)\n",
    "# model.summary()\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\n",
    "#     \"accuracy\",\n",
    "# ])\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=EMBED_DIMENSION, \n",
    "                     embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
    "                     input_length=SEQUENCE_LENGTH, trainable = False))\n",
    "model.add(layers.Permute((2, 1)))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu', name=\"Conv1D\"))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, name=\"MaxPooling1D\"))\n",
    "# model.add(layers.LSTM(32, activation=None, name=\"LSTM\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\n",
    "    \"accuracy\",\n",
    "    tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = token.texts_to_sequences(train[column_names[0]])\n",
    "train_x = keras.preprocessing.sequence.pad_sequences(train_x, maxlen=SEQUENCE_LENGTH, padding='post', truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(x=train_x, y=train[column_names[1]], validation_split=0.2,\n",
    "          epochs=epochs, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = token.texts_to_sequences(test[column_names[0]])\n",
    "test_x = keras.preprocessing.sequence.pad_sequences(test_x, maxlen=SEQUENCE_LENGTH, padding='post', truncating=\"post\")\n",
    "loss, accuracy, precision, recall = model.evaluate(x=test_x, y=test[column_names[1]])\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CNN_ISOT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
