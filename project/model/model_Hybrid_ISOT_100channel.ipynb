{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3768376842654507496\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1549271040\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15813315496928167029\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_VNUM = 20000\n",
    "SEQUENCE_LENGTH = 300\n",
    "EMBED_DIMENSION = 100\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = [\"article_content\", \"labels\", \"article_title\"]\n",
    "# data = pd.read_csv(\"../FAKES_Dataset/FA-KES-Dataset.csv\", encoding='utf-8', encoding_errors='ignore')[column_names]\n",
    "# data[\"article_content\"] = data[\"article_title\"] + \" \" + data[\"article_content\"]\n",
    "# train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"text\", \"labels\", \"title\"]\n",
    "data = pd.read_csv(\"../ISOT_Dataset/ISOT_Dataset.csv\", encoding='utf-8', encoding_errors='ignore')[column_names]\n",
    "# data[\"text\"] = data[\"title\"] + \" \" + data[\"text\"]\n",
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = keras.preprocessing.text.Tokenizer()\n",
    "token.fit_on_texts(data[column_names[0]])\n",
    "vocab_size = len(token.word_index) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:08, 44455.27it/s]\n",
      "100%|██████████| 138021/138021 [00:00<00:00, 850607.25it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = {}\n",
    "f = open('../data/glove.6B.100d.txt')\n",
    "# for line in tqdm(f):\n",
    "#     value = line.split(' ')\n",
    "#     word = value[0]\n",
    "#     coef = np.array(value[1:],dtype = 'float32')\n",
    "#     embedding_vector[word] = coef\n",
    "for line in tqdm(f):\n",
    "    word, coefs = line.split(maxsplit=1)\n",
    "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "    embedding_vector[word] = coefs\n",
    "embedding_matrix = np.zeros((vocab_size, EMBED_DIMENSION))\n",
    "for word,i in tqdm(token.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer LSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          13802300  \n",
      "_________________________________________________________________\n",
      "Conv1D (Conv1D)              (None, 296, 128)          64128     \n",
      "_________________________________________________________________\n",
      "MaxPooling1D (MaxPooling1D)  (None, 148, 128)          0         \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,887,069\n",
      "Trainable params: 84,769\n",
      "Non-trainable params: 13,802,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inputs = keras.Input(shape=(None,), name=\"input\")\n",
    "# x = layers.Embedding(input_dim=vocab_size, output_dim=EMBED_DIMENSION, \n",
    "#                      embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
    "#                      input_length=SEQUENCE_LENGTH, trainable = False)(inputs)\n",
    "# x = layers.Conv1D(128, 5, activation='relu', use_bias=False, name=\"Conv1D\")(x)\n",
    "# x = layers.MaxPooling1D(pool_size=2, name=\"MaxPooling1D\")(x)\n",
    "# x = layers.LSTM(32, activation=None, use_bias=False, name=\"LSTM\")(x)\n",
    "# outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "# model = keras.Model(inputs, outputs)\n",
    "# model.summary()\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\n",
    "#     \"accuracy\",\n",
    "# ])\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=EMBED_DIMENSION, \n",
    "                     embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
    "                     input_length=SEQUENCE_LENGTH, trainable = False))\n",
    "# model.add(layers.Permute((2, 1)))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu', name=\"Conv1D\"))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, name=\"MaxPooling1D\"))\n",
    "model.add(layers.LSTM(32, activation=None, name=\"LSTM\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\n",
    "    \"accuracy\",\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = token.texts_to_sequences(train[column_names[0]])\n",
    "train_x = keras.preprocessing.sequence.pad_sequences(train_x, maxlen=SEQUENCE_LENGTH, padding='post', truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "449/449 [==============================] - 160s 334ms/step - loss: 2.8375 - accuracy: 0.6754 - val_loss: 0.5698 - val_accuracy: 0.6936\n",
      "Epoch 2/10\n",
      "449/449 [==============================] - 139s 310ms/step - loss: 0.5643 - accuracy: 0.7096 - val_loss: 0.5655 - val_accuracy: 0.6992\n",
      "Epoch 3/10\n",
      "449/449 [==============================] - 129s 287ms/step - loss: 0.5601 - accuracy: 0.7101 - val_loss: 0.5614 - val_accuracy: 0.7013\n",
      "Epoch 4/10\n",
      "449/449 [==============================] - 138s 308ms/step - loss: 0.5556 - accuracy: 0.7112 - val_loss: 0.5569 - val_accuracy: 0.7027\n",
      "Epoch 5/10\n",
      "449/449 [==============================] - 144s 321ms/step - loss: 0.5510 - accuracy: 0.7124 - val_loss: 0.5524 - val_accuracy: 0.7031\n",
      "Epoch 6/10\n",
      "449/449 [==============================] - 147s 327ms/step - loss: 0.5465 - accuracy: 0.7131 - val_loss: 0.5483 - val_accuracy: 0.7048\n",
      "Epoch 7/10\n",
      "449/449 [==============================] - 156s 348ms/step - loss: 0.5423 - accuracy: 0.7143 - val_loss: 0.5445 - val_accuracy: 0.7045\n",
      "Epoch 8/10\n",
      "449/449 [==============================] - 150s 335ms/step - loss: 0.5384 - accuracy: 0.7137 - val_loss: 0.5414 - val_accuracy: 0.7057\n",
      "Epoch 9/10\n",
      "449/449 [==============================] - 138s 308ms/step - loss: 0.5352 - accuracy: 0.7149 - val_loss: 0.5385 - val_accuracy: 0.7077\n",
      "Epoch 10/10\n",
      "449/449 [==============================] - 159s 354ms/step - loss: 0.5327 - accuracy: 0.7157 - val_loss: 0.5360 - val_accuracy: 0.7087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1802f63ce50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "model.fit(x=train_x, y=train[column_names[1]], validation_split=0.2,\n",
    "          epochs=epochs, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 9s 32ms/step - loss: 0.5351 - accuracy: 0.7098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5351209044456482, 0.7097995281219482]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = token.texts_to_sequences(test[column_names[0]])\n",
    "test_x = keras.preprocessing.sequence.pad_sequences(test_x, maxlen=SEQUENCE_LENGTH, padding='post', truncating=\"post\")\n",
    "model.evaluate(x=test_x, y=test[column_names[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
